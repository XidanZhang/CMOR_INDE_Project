# Other Ensemble Methods: Boosting (AdaBoost and Gradient Boosting)

## Introduction
Boosting is an ensemble learning method that combines weak learners into a strong learner iteratively. This implementation uses `AdaBoostClassifier` (with the SAMME algorithm) and `GradientBoostingClassifier` from scikit-learn to perform binary classification.

---

## Dataset
The dataset is synthetically generated using scikit-learn's `make_classification`. It contains two features and one binary label.

- **Number of samples**: 300  
- **Number of features**: 2  
- **Classes**: 0 and 1  

---

## Ensemble Methods
### AdaBoost (with SAMME Algorithm)
- Combines multiple weak classifiers (decision stumps) into a strong classifier.
- **SAMME Algorithm**:
  - Adjusts sample weights after each weak learner.
  - Focuses more on misclassified samples in subsequent iterations.
- **Key Parameters**:
  - Number of estimators: 50
  - Learning rate: 1.0

### Gradient Boosting
- Iteratively minimizes a loss function by training decision trees.
- Uses gradient descent to optimize predictions, making it more precise than AdaBoost.
- **Key Parameters**:
  - Number of estimators: 100
  - Learning rate: 0.1
  - Maximum depth: 3

---

## Training Process
- **Train-Test Split**: 80% training, 20% testing  
- **Performance Metrics**:
  - Accuracy
  - Classification Report

Classification Report (AdaBoost): precision recall f1-score support
       0       0.88      0.92      0.90        29
       1       0.93      0.89      0.91        31

accuracy                           0.90        60

Classification Report (Gradient Boosting): precision recall f1-score support
       0       0.90      0.93      0.92        29
       1       0.94      0.90      0.92        31

accuracy                           0.92        60


---

## Results
### 1. Decision Boundary
- **AdaBoost Decision Boundary (SAMME)**:
  - Visualizes the classification regions generated by AdaBoost with the SAMME algorithm.
- **Gradient Boosting Decision Boundary**:
  - Visualizes the classification regions generated by Gradient Boosting.

### 2. Feature Importance
- Feature importance in Gradient Boosting is visualized as a bar chart, ranking the features by their contribution to the model's predictions.



