# Random Forest Algorithm

## Introduction
Random Forests are an ensemble learning method that combines the predictions of multiple decision trees to produce a more robust and accurate model. This implementation uses the `RandomForestClassifier` from scikit-learn to perform binary classification.

---

## Dataset
The dataset is synthetically generated using scikit-learn's `make_classification`. It contains two features and one binary label.

- **Number of samples**: 300  
- **Number of features**: 2  
- **Classes**: 0 and 1  

---

## Random Forest Implementation
### 1. Core Concepts
- **Bootstrap Aggregation**:
  - Each tree is trained on a random subset of the data with replacement (bagging).
- **Feature Subset**:
  - At each split, only a random subset of features is considered.
- **Voting**:
  - The final prediction is based on the majority vote of all trees.

### 2. Model Parameters
- **Number of Trees (n_estimators)**: 100  
- **Max Depth**: 5  

---

## Training Process
- **Train-Test Split**: 80% training, 20% testing  
- **Performance Metrics**:
  - Accuracy
  - Classification Report
  - Confusion Matrix

       0       0.95      0.91      0.93        29
       1       0.92      0.96      0.94        31

accuracy                           0.93        60


---

## Results
### 1. Decision Boundary
The decision boundary illustrates the classification regions generated by the Random Forest model.

### 2. Confusion Matrix
The confusion matrix visualizes the performance of the model in terms of true positives, false positives, true negatives, and false negatives.

### 3. Feature Importance
The feature importance bar chart ranks the features based on their contribution to the model's performance.



